{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classes.network import Network\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import math\n",
    "\n",
    "def shuffle_and_split(X_train, y_train, batch_size=10):\n",
    "    \n",
    "    number_of_chunks_to_split = math.ceil(len(y_train)/batch_size)\n",
    "    \n",
    "    p = np.random.permutation(len(X_train))\n",
    "\n",
    "    new_X_train = X_train[p]\n",
    "    new_y_train = y_train[p]\n",
    "\n",
    "    splitted_X_train = np.array_split(new_X_train, number_of_chunks_to_split)\n",
    "    splitted_y_train = np.array_split(new_y_train, number_of_chunks_to_split)\n",
    "\n",
    "    return splitted_X_train[:-1], splitted_y_train[:-1], splitted_X_train[-1], splitted_y_train[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shallow vs Deep neural networks\n",
    "\n",
    "When training neural networks, we have to come up with a design that fits its intention. Here, we tried to find, given a certain dataset, which architecture was the most ideal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILES = [\n",
    "    \"data/1in_tanh.txt\",\n",
    "    \"data/1in_linear.txt\",\n",
    "    \"data/1in_cubic.txt\",\n",
    "    \"data/1in_sine.txt\",\n",
    "    \"data/2in_complex.txt\",\n",
    "    \"data/2in_xor.txt\"\n",
    "]\n",
    "def load_dataset(path):\n",
    "    ds = pd.read_csv(path, sep=r\"\\s+\", header=None)\n",
    "    X_train, y_train = ds.iloc[:,:-1].to_numpy(), ds.iloc[:,-1].to_numpy()\n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "\tTraining shallow on 1in_tan\n",
      "Best_fitness: 9.188523134800781e-05\n",
      "Best_fitness: 3.374437183902416e-09\n",
      "Best_fitness: 9.833583102853636e-07\n",
      "Best_fitness: 2.22738615205029e-05\n",
      "Best_fitness: 4.0975085147504436e-06\n",
      "Error on test: 5.843478612375912e-06\n",
      "Time: 234.34072470664978\n",
      "-----------------------------\n",
      "\tTraining deep on 1in_tan\n",
      "Best_fitness: 0.06742680086555371\n",
      "Best_fitness: 0.008458325341320901\n",
      "Best_fitness: 0.013231064894961201\n",
      "Best_fitness: 0.06647012667079867\n",
      "Best_fitness: 0.03515286289902064\n",
      "Error on test: 0.17911776604604113\n",
      "Time: 531.8018336296082\n",
      "-----------------------------\n",
      "\tTraining shallow on 1in_linear\n",
      "Best_fitness: 0.004994995228089439\n",
      "Best_fitness: 0.0013646573717595724\n",
      "Best_fitness: 0.0021481531866028945\n",
      "Best_fitness: 0.00286434417232185\n",
      "Error on test: 0.00482097115333959\n",
      "Time: 181.5251317024231\n",
      "-----------------------------\n",
      "\tTraining deep on 1in_linear\n",
      "Best_fitness: 0.03314011438444224\n",
      "Best_fitness: 0.014510346876163432\n",
      "Best_fitness: 0.027319944171004133\n",
      "Best_fitness: 0.03350278639446248\n",
      "Error on test: 0.06586227335212336\n",
      "Time: 419.62034845352173\n",
      "-----------------------------\n",
      "\tTraining shallow on 1in_cubic\n",
      "Best_fitness: 0.03692253953246003\n",
      "Best_fitness: 0.01403957795320125\n",
      "Best_fitness: 0.03242895685785711\n",
      "Best_fitness: 0.024902613676759146\n",
      "Best_fitness: 0.013613187338883956\n",
      "Error on test: 0.02628952348551735\n",
      "Time: 193.6641411781311\n",
      "-----------------------------\n",
      "\tTraining deep on 1in_cubic\n",
      "Best_fitness: 0.07586365937845391\n",
      "Best_fitness: 0.028386472710529683\n",
      "Best_fitness: 0.04639527452226233\n",
      "Best_fitness: 0.040553048045728894\n",
      "Best_fitness: 0.018831276864198953\n",
      "Error on test: 0.027876169640711494\n",
      "Time: 452.29343724250793\n",
      "-----------------------------\n",
      "\tTraining shallow on 1in_sine\n",
      "Best_fitness: 9.84437049516384e-05\n",
      "Best_fitness: 0.00011117679611286874\n",
      "Best_fitness: 7.260584797048707e-05\n",
      "Best_fitness: 7.613077996101073e-05\n",
      "Error on test: 8.649286769624107e-05\n",
      "Time: 188.0481505393982\n",
      "-----------------------------\n",
      "\tTraining deep on 1in_sine\n",
      "Best_fitness: 0.04996931733678212\n",
      "Best_fitness: 0.10095065939750947\n",
      "Best_fitness: 0.09205041336631571\n",
      "Best_fitness: 0.0731940194671301\n",
      "Error on test: 0.13241142698369202\n",
      "Time: 436.8915390968323\n",
      "-----------------------------\n",
      "\tTraining shallow on 2in_complex\n",
      "Best_fitness: 0.05569128682378492\n",
      "Best_fitness: 0.09593669223980321\n",
      "Best_fitness: 0.03935732119379402\n",
      "Best_fitness: 0.11345672850590423\n",
      "Error on test: 0.18620737972724138\n",
      "Time: 188.31769347190857\n",
      "-----------------------------\n",
      "\tTraining deep on 2in_complex\n",
      "Best_fitness: 0.05385408546223168\n",
      "Best_fitness: 0.10086549473775122\n",
      "Best_fitness: 0.040619073525740276\n",
      "Best_fitness: 0.119302860868245\n",
      "Error on test: 0.20563523481126667\n",
      "Time: 425.55572152137756\n",
      "-----------------------------\n",
      "\tTraining shallow on 2in_xor\n",
      "Best_fitness: 0.23796172604617521\n",
      "Best_fitness: 0.0008564128202882288\n",
      "Best_fitness: 0.1500000431382191\n",
      "Best_fitness: 0.1500107882546476\n",
      "Error on test: 0.15001688883871395\n",
      "Time: 213.37003588676453\n",
      "-----------------------------\n",
      "\tTraining deep on 2in_xor\n",
      "Best_fitness: 0.018876908218650616\n",
      "Best_fitness: 0.003215241434136113\n",
      "Best_fitness: 0.018350751452251197\n",
      "Best_fitness: 0.004772428309042237\n",
      "Error on test: 0.0040653348274935965\n",
      "Time: 466.45396399497986\n"
     ]
    }
   ],
   "source": [
    "result_networks_n = {\n",
    "    \"1in_tan\":{},\n",
    "    \"1in_linear\":{},\n",
    "    \"1in_cubic\":{},\n",
    "    \"1in_sine\":{},\n",
    "    \"2in_complex\":{},\n",
    "    \"2in_xor\":{}\n",
    "}\n",
    "\n",
    "#batch:20,swarm:100,iter:10\n",
    "for i, dataset in enumerate(result_networks_n.keys()):\n",
    "    #Loading the dataset\n",
    "    X_train, y_train = load_dataset(DATA_FILES[i])\n",
    "    splitted_X_train, splitted_y_train, X_test, y_test = shuffle_and_split(X_train, y_train, batch_size=20)\n",
    "    \n",
    "    # First, creating a shallow network\n",
    "    shallow = Network(error=\"l2\")\n",
    "    shallow.addLayer(1, inputSize=eval(dataset[0]), activation=\"tanh\")\n",
    "    print('-----------------------------')\n",
    "    print(f\"\\tTraining shallow on {dataset}\")\n",
    "    \n",
    "    start_shallow = time.time()\n",
    "    shallow_train_perfs = []\n",
    "    for (x_batch,y_batch) in zip(splitted_X_train,splitted_y_train):\n",
    "        perf = shallow.train_with_pso(x_batch, y_batch)\n",
    "        shallow_train_perfs.append(perf)\n",
    "        \n",
    "    shallow_test = shallow.test(X_test, y_test)\n",
    "    print(f\"Error on test: {shallow_test}\")\n",
    "    end_shallow = time.time()\n",
    "        \n",
    "    print(f\"Time: {end_shallow - start_shallow}\")\n",
    "    \n",
    "    start_deep = time.time()\n",
    "    # Then, creating a deep network\n",
    "    deep = Network(error=\"l2\")\n",
    "    deep.addLayer(3, inputSize=eval(dataset[0]), activation=\"tanh\")\n",
    "    deep.addLayer(3, activation=\"tanh\")\n",
    "    deep.addLayer(1, activation=\"tanh\")\n",
    "    print('-----------------------------')\n",
    "    print(f\"\\tTraining deep on {dataset}\")\n",
    "    \n",
    "    deep_train_perfs = []\n",
    "    for (x_batch,y_batch) in zip(splitted_X_train,splitted_y_train):\n",
    "        perf = deep.train_with_pso(x_batch, y_batch)\n",
    "        deep_train_perfs.append(perf)\n",
    "        \n",
    "    deep_test = deep.test(X_test, y_test)\n",
    "    print(f\"Error on test: {deep_test}\")\n",
    "    end_deep = time.time()\n",
    "    \n",
    "    print(f\"Time: {end_deep - start_deep}\")\n",
    "    \n",
    "    result_networks_n[dataset][\"shallow\"] = {\n",
    "        \"model\": shallow,\n",
    "        \"train_perfs\": shallow_train_perfs,\n",
    "        \"test_perfs\": shallow_test,\n",
    "        \"exec_time\": end_shallow - start_shallow\n",
    "    }\n",
    "    \n",
    "    result_networks_n[dataset][\"deep\"] = {\n",
    "        \"model\": deep,\n",
    "        \"train_perfs\": deep_train_perfs,\n",
    "        \"test_perfs\": deep_test,\n",
    "        \"exec_time\": end_deep - start_deep\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For dataset '1in_tan':\n",
      "\tSHALLOW\n",
      "Test perfs: 5.843478612375912e-06\n",
      "Exec time: 234.34072470664978\n",
      "\tDEEP\n",
      "Test perfs: 0.17911776604604113\n",
      "Exec time: 531.8018336296082\n",
      " -----------------\n",
      "\n",
      "For dataset '1in_linear':\n",
      "\tSHALLOW\n",
      "Test perfs: 0.00482097115333959\n",
      "Exec time: 181.5251317024231\n",
      "\tDEEP\n",
      "Test perfs: 0.06586227335212336\n",
      "Exec time: 419.62034845352173\n",
      " -----------------\n",
      "\n",
      "For dataset '1in_cubic':\n",
      "\tSHALLOW\n",
      "Test perfs: 0.02628952348551735\n",
      "Exec time: 193.6641411781311\n",
      "\tDEEP\n",
      "Test perfs: 0.027876169640711494\n",
      "Exec time: 452.29343724250793\n",
      " -----------------\n",
      "\n",
      "For dataset '1in_sine':\n",
      "\tSHALLOW\n",
      "Test perfs: 8.649286769624107e-05\n",
      "Exec time: 188.0481505393982\n",
      "\tDEEP\n",
      "Test perfs: 0.13241142698369202\n",
      "Exec time: 436.8915390968323\n",
      " -----------------\n",
      "\n",
      "For dataset '2in_complex':\n",
      "\tSHALLOW\n",
      "Test perfs: 0.18620737972724138\n",
      "Exec time: 188.31769347190857\n",
      "\tDEEP\n",
      "Test perfs: 0.20563523481126667\n",
      "Exec time: 425.55572152137756\n",
      " -----------------\n",
      "\n",
      "For dataset '2in_xor':\n",
      "\tSHALLOW\n",
      "Test perfs: 0.15001688883871395\n",
      "Exec time: 213.37003588676453\n",
      "\tDEEP\n",
      "Test perfs: 0.0040653348274935965\n",
      "Exec time: 466.45396399497986\n",
      " -----------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, dataset in enumerate(result_networks_n.keys()):\n",
    "    print(f\"For dataset '{dataset}':\")\n",
    "    print(\"\\tSHALLOW\")\n",
    "    print(f\"Test perfs: {result_networks_n[dataset]['shallow']['test_perfs']}\")\n",
    "    print(f\"Exec time: {result_networks_n[dataset]['shallow']['exec_time']}\")\n",
    "          \n",
    "    print(\"\\tDEEP\")\n",
    "    print(f\"Test perfs: {result_networks_n[dataset]['deep']['test_perfs']}\")\n",
    "    print(f\"Exec time: {result_networks_n[dataset]['deep']['exec_time']}\")\n",
    "    print(\"\",\"-----------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying to improve Deep architecture training by tuning PSO parameters such as swarm size and iterations\n",
    "\n",
    "Giving PSO the right tools for it to reach better convergence is the reason that motivates these investigations ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "\tTraining shallow on 1in_tan\n",
      "Best_fitness: 5.785347652660321e-10\n",
      "Best_fitness: 4.881118709392624e-10\n",
      "Error on test: 1.1833323767106554e-09\n",
      "-----------------------------\n",
      "\tTraining deep on 1in_tan\n",
      "Time: 1595.451779127121\n",
      "Best_fitness: 0.053264869330752765\n",
      "Best_fitness: 0.0037893873318501784\n",
      "Error on test: 0.07587599468992168\n",
      "Time: 3913.156039237976\n",
      "-----------------------------\n",
      "\tTraining shallow on 1in_linear\n",
      "Best_fitness: 0.004102476887832322\n",
      "Error on test: 0.0026793372467690197\n",
      "-----------------------------\n",
      "\tTraining deep on 1in_linear\n",
      "Time: 1146.9014484882355\n",
      "Best_fitness: 0.019793313275948767\n",
      "Error on test: 0.01393165556123718\n",
      "Time: 2661.0522117614746\n",
      "-----------------------------\n",
      "\tTraining shallow on 1in_cubic\n",
      "Best_fitness: 0.026975802651453676\n",
      "Best_fitness: 0.02206765289505194\n",
      "Error on test: 0.03285611323714251\n",
      "-----------------------------\n",
      "\tTraining deep on 1in_cubic\n",
      "Time: 1517.082524061203\n",
      "Best_fitness: 0.031112137103559076\n",
      "Best_fitness: 0.03290936928445983\n",
      "Error on test: 0.04639195586733256\n",
      "Time: 3597.5754783153534\n",
      "-----------------------------\n",
      "\tTraining shallow on 1in_sine\n",
      "Best_fitness: 2.4335762437371604e-08\n",
      "Error on test: 2.36671663269262e-08\n",
      "-----------------------------\n",
      "\tTraining deep on 1in_sine\n",
      "Time: 1110.8622043132782\n",
      "Best_fitness: 0.05859965600757905\n",
      "Error on test: 0.08036977440141538\n",
      "Time: 2582.4182965755463\n",
      "-----------------------------\n",
      "\tTraining shallow on 2in_complex\n",
      "Best_fitness: 0.06773685144520628\n",
      "Error on test: 0.13966536411015082\n",
      "-----------------------------\n",
      "\tTraining deep on 2in_complex\n",
      "Time: 1122.8274807929993\n",
      "Best_fitness: 0.07511525790337058\n",
      "Error on test: 0.15853007848335715\n",
      "Time: 2668.7885870933533\n",
      "-----------------------------\n",
      "\tTraining shallow on 2in_xor\n",
      "Best_fitness: 4.087605875796119e-08\n",
      "Error on test: 4.417952225623807e-08\n",
      "-----------------------------\n",
      "\tTraining deep on 2in_xor\n",
      "Time: 1177.7592322826385\n",
      "Best_fitness: 0.0\n",
      "Error on test: 0.0\n",
      "Time: 2651.615844488144\n"
     ]
    }
   ],
   "source": [
    "result_networks_custom = {\n",
    "    \"1in_tan\":{},\n",
    "    \"1in_linear\":{},\n",
    "    \"1in_cubic\":{},\n",
    "    \"1in_sine\":{},\n",
    "    \"2in_complex\":{},\n",
    "    \"2in_xor\":{}\n",
    "}\n",
    "\n",
    "#batch:50,swarm:100,iter:100\n",
    "for i, dataset in enumerate(result_networks_custom.keys()):\n",
    "    #Loading the dataset\n",
    "    X_train, y_train = load_dataset(DATA_FILES[i])\n",
    "    splitted_X_train, splitted_y_train, X_test, y_test = shuffle_and_split(X_train, y_train, batch_size=50)\n",
    "    \n",
    "    # First, creating a shallow network\n",
    "    shallow = Network(error=\"l2\")\n",
    "    shallow.addLayer(1, inputSize=eval(dataset[0]), activation=\"tanh\")\n",
    "    print('-----------------------------')\n",
    "    print(f\"\\tTraining shallow on {dataset}\")\n",
    "    \n",
    "    start_shallow = time.time()\n",
    "    shallow_train_perfs = []\n",
    "    for (x_batch,y_batch) in zip(splitted_X_train,splitted_y_train):\n",
    "        perf = shallow.train_with_pso(x_batch, y_batch, iter_count=100)\n",
    "        shallow_train_perfs.append(perf)\n",
    "        \n",
    "    shallow_test = shallow.test(X_test, y_test)\n",
    "    print(f\"Error on test: {shallow_test}\")\n",
    "    end_shallow = time.time()\n",
    "    \n",
    "    start_deep = time.time()\n",
    "    # Then, creating a deep network\n",
    "    deep = Network(error=\"l2\")\n",
    "    deep.addLayer(3, inputSize=eval(dataset[0]), activation=\"tanh\")\n",
    "    deep.addLayer(3, activation=\"tanh\")\n",
    "    deep.addLayer(1, activation=\"tanh\")\n",
    "    print('-----------------------------')\n",
    "    print(f\"\\tTraining deep on {dataset}\")\n",
    "        \n",
    "    print(f\"Time: {end_shallow - start_shallow}\")\n",
    "    \n",
    "    deep_train_perfs = []\n",
    "    for (x_batch,y_batch) in zip(splitted_X_train,splitted_y_train):\n",
    "        perf = deep.train_with_pso(x_batch, y_batch, iter_count=100)\n",
    "        deep_train_perfs.append(perf)\n",
    "        \n",
    "    deep_test = deep.test(X_test, y_test)\n",
    "    print(f\"Error on test: {deep_test}\")\n",
    "    end_deep = time.time()\n",
    "    \n",
    "    print(f\"Time: {end_deep - start_deep}\")\n",
    "    \n",
    "    result_networks_custom[dataset][\"shallow\"] = {\n",
    "        \"model\": shallow,\n",
    "        \"train_perfs\": shallow_train_perfs,\n",
    "        \"test_perfs\": shallow_test,\n",
    "        \"exec_time\": end_shallow - start_shallow\n",
    "    }\n",
    "    \n",
    "    result_networks_custom[dataset][\"deep\"] = {\n",
    "        \"model\": deep,\n",
    "        \"train_perfs\": deep_train_perfs,\n",
    "        \"test_perfs\": deep_test,\n",
    "        \"exec_time\": end_deep - start_deep\n",
    "    }\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For dataset '1in_tan':\n",
      "\tSHALLOW\n",
      "Test perfs: 1.1833323767106554e-09\n",
      "Exec time: 1595.451779127121\n",
      "\tDEEP\n",
      "Test perfs: 0.07587599468992168\n",
      "Exec time: 3913.156039237976\n",
      " -----------------\n",
      "\n",
      "For dataset '1in_linear':\n",
      "\tSHALLOW\n",
      "Test perfs: 0.0026793372467690197\n",
      "Exec time: 1146.9014484882355\n",
      "\tDEEP\n",
      "Test perfs: 0.01393165556123718\n",
      "Exec time: 2661.0522117614746\n",
      " -----------------\n",
      "\n",
      "For dataset '1in_cubic':\n",
      "\tSHALLOW\n",
      "Test perfs: 0.03285611323714251\n",
      "Exec time: 1517.082524061203\n",
      "\tDEEP\n",
      "Test perfs: 0.04639195586733256\n",
      "Exec time: 3597.5754783153534\n",
      " -----------------\n",
      "\n",
      "For dataset '1in_sine':\n",
      "\tSHALLOW\n",
      "Test perfs: 2.36671663269262e-08\n",
      "Exec time: 1110.8622043132782\n",
      "\tDEEP\n",
      "Test perfs: 0.08036977440141538\n",
      "Exec time: 2582.4182965755463\n",
      " -----------------\n",
      "\n",
      "For dataset '2in_complex':\n",
      "\tSHALLOW\n",
      "Test perfs: 0.13966536411015082\n",
      "Exec time: 1122.8274807929993\n",
      "\tDEEP\n",
      "Test perfs: 0.15853007848335715\n",
      "Exec time: 2668.7885870933533\n",
      " -----------------\n",
      "\n",
      "For dataset '2in_xor':\n",
      "\tSHALLOW\n",
      "Test perfs: 4.417952225623807e-08\n",
      "Exec time: 1177.7592322826385\n",
      "\tDEEP\n",
      "Test perfs: 0.0\n",
      "Exec time: 2651.615844488144\n",
      " -----------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, dataset in enumerate(result_networks_custom.keys()):\n",
    "    print(f\"For dataset '{dataset}':\")\n",
    "    print(\"\\tSHALLOW\")\n",
    "    print(f\"Test perfs: {result_networks_custom[dataset]['shallow']['test_perfs']}\")\n",
    "    print(f\"Exec time: {result_networks_custom[dataset]['shallow']['exec_time']}\")\n",
    "          \n",
    "    print(\"\\tDEEP\")\n",
    "    print(f\"Test perfs: {result_networks_custom[dataset]['deep']['test_perfs']}\")\n",
    "    print(f\"Exec time: {result_networks_custom[dataset]['deep']['exec_time']}\")\n",
    "    print(\"\",\"-----------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying to improve PSO convergence with updated velocity function\n",
    "\n",
    "We updated our velocity function and we will check the performance of our PSO algorithm then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "\tTraining shallow on 1in_tan\n",
      "Best_fitness: 5.76877983434113e-06\n",
      "Best_fitness: 2.2165358258332538e-05\n",
      "Best_fitness: 0.00012983721309977591\n",
      "Best_fitness: 0.00011962907460070403\n",
      "Best_fitness: 0.0006630407597474645\n",
      "Error on test: 0.00038940707669185516\n",
      "-----------------------------\n",
      "\tTraining deep on 1in_tan\n",
      "Time: 2011.7609379291534\n",
      "Best_fitness: 0.021039456941624072\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-7bfec9290574>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0my_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[0miter_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m             increase_convergence_factor=True)\n\u001b[0m\u001b[0;32m     54\u001b[0m         \u001b[0mdeep_train_perfs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mperf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\neural_networks_with_pso\\classes\\network.py\u001b[0m in \u001b[0;36mtrain_with_pso\u001b[1;34m(self, X_train, y_train, swarm_size, iter_count, increase_convergence_factor)\u001b[0m\n\u001b[0;32m    279\u001b[0m         \u001b[0mpso\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPSO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswarm_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m         \u001b[1;31m#for x, y in tqdm(zip(X_train, y_train)):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 281\u001b[1;33m         \u001b[0mnetwork\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mperf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpso\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_nn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_time\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0miter_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mincrease_convergence_factor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mincrease_convergence_factor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    282\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mperf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\neural_networks_with_pso\\classes\\pso.py\u001b[0m in \u001b[0;36mtrain_nn\u001b[1;34m(self, X_train, y_train, max_time, threshold, increase_convergence_factor)\u001b[0m\n\u001b[0;32m    139\u001b[0m                     \u001b[0minformant\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mP\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"location\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"l{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m                     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massessFitness\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minformant\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massessFitness\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_informant\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m                         \u001b[0mbest_informant\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mP\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"location\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"l{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\neural_networks_with_pso\\classes\\pso.py\u001b[0m in \u001b[0;36massessFitness\u001b[1;34m(self, particule, y_train, X_train, is_best)\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0massessFitness\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparticule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_best\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m         \u001b[0mnew_net\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetNetworkFromParticule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparticule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\neural_networks_with_pso\\classes\\pso.py\u001b[0m in \u001b[0;36mgetNetworkFromParticule\u001b[1;34m(self, particuleValue)\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[0mb_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"biases\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"b{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m             \u001b[0mnew_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparticuleValue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcompteur\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcompteur\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m             \u001b[0mbias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparticuleValue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcompteur\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcompteur\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mb_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0mact_fun\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparticuleValue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcompteur\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mb_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcompteur\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mb_size\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result_networks_new_v = {\n",
    "    \"1in_tan\":{},\n",
    "    \"1in_linear\":{},\n",
    "    \"1in_cubic\":{},\n",
    "    \"1in_sine\":{},\n",
    "    \"2in_complex\":{},\n",
    "    \"2in_xor\":{}\n",
    "}\n",
    "\n",
    "#batch:20,swarm:100,iter:100\n",
    "for i, dataset in enumerate(result_networks_new_v.keys()):\n",
    "    #Loading the dataset\n",
    "    X_train, y_train = load_dataset(DATA_FILES[i])\n",
    "    splitted_X_train, splitted_y_train, X_test, y_test = shuffle_and_split(X_train, y_train, batch_size=20)\n",
    "    \n",
    "    # First, creating a shallow network\n",
    "    shallow = Network(error=\"l2\")\n",
    "    shallow.addLayer(1, inputSize=eval(dataset[0]), activation=\"tanh\")\n",
    "    print('-----------------------------')\n",
    "    print(f\"\\tTraining shallow on {dataset}\")\n",
    "    \n",
    "    start_shallow = time.time()\n",
    "    shallow_train_perfs = []\n",
    "    for (x_batch,y_batch) in zip(splitted_X_train,splitted_y_train):\n",
    "        perf = shallow.train_with_pso(\n",
    "            x_batch, \n",
    "            y_batch, \n",
    "            iter_count=100,\n",
    "            increase_convergence_factor=True)\n",
    "        shallow_train_perfs.append(perf)\n",
    "        \n",
    "    shallow_test = shallow.test(X_test, y_test)\n",
    "    print(f\"Error on test: {shallow_test}\")\n",
    "    end_shallow = time.time()\n",
    "    \n",
    "    start_deep = time.time()\n",
    "    # Then, creating a deep network\n",
    "    deep = Network(error=\"l2\")\n",
    "    deep.addLayer(3, inputSize=eval(dataset[0]), activation=\"tanh\")\n",
    "    deep.addLayer(3, activation=\"tanh\")\n",
    "    deep.addLayer(1, activation=\"tanh\")\n",
    "    print('-----------------------------')\n",
    "    print(f\"\\tTraining deep on {dataset}\")\n",
    "        \n",
    "    print(f\"Time: {end_shallow - start_shallow}\")\n",
    "    \n",
    "    deep_train_perfs = []\n",
    "    for (x_batch,y_batch) in zip(splitted_X_train,splitted_y_train):\n",
    "        perf = deep.train_with_pso(\n",
    "            x_batch, \n",
    "            y_batch,\n",
    "            iter_count=100,\n",
    "            increase_convergence_factor=True)\n",
    "        deep_train_perfs.append(perf)\n",
    "        \n",
    "    deep_test = deep.test(X_test, y_test)\n",
    "    print(f\"Error on test: {deep_test}\")\n",
    "    end_deep = time.time()\n",
    "    \n",
    "    print(f\"Time: {end_deep - start_deep}\")\n",
    "    \n",
    "    result_networks_new_v[dataset][\"shallow\"] = {\n",
    "        \"model\": shallow,\n",
    "        \"train_perfs\": shallow_train_perfs,\n",
    "        \"test_perfs\": shallow_test,\n",
    "        \"exec_time\": end_shallow - start_shallow\n",
    "    }\n",
    "    \n",
    "    result_networks_new_v[dataset][\"deep\"] = {\n",
    "        \"model\": deep,\n",
    "        \"train_perfs\": deep_train_perfs,\n",
    "        \"test_perfs\": deep_test,\n",
    "        \"exec_time\": end_deep - start_deep\n",
    "    }\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, dataset in enumerate(result_networks_new_v.keys()):\n",
    "    print(f\"For dataset '{dataset}':\")\n",
    "    print(\"\\tSHALLOW\")\n",
    "    print(f\"Test perfs: {result_networks_new_v[dataset]['shallow']['test_perfs']}\")\n",
    "    print(f\"Exec time: {result_networks_new_v[dataset]['shallow']['exec_time']}\")\n",
    "          \n",
    "    print(\"\\tDEEP\")\n",
    "    print(f\"Test perfs: {result_networks_new_v[dataset]['deep']['test_perfs']}\")\n",
    "    print(f\"Exec time: {result_networks_new_v[dataset]['deep']['exec_time']}\")\n",
    "    print(\"\",\"-----------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots\n",
    "We will now plot the results in graph and compare them to the original function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(-5,5,1000)\n",
    "\n",
    "actual_functions = [\n",
    "    lambda x: np.tanh(x),\n",
    "    lambda x: x,\n",
    "    lambda x: x*x*x,\n",
    "    lambda x: np.sin(x)\n",
    "    \n",
    "]\n",
    "\n",
    "for i, dataset in enumerate(result_networks_new_v.keys()):\n",
    "    if (i < 4):\n",
    "        shallow_res = result_networks_new_v[dataset]['shallow']['model'].predict(x)\n",
    "        deep_res = result_networks_new_v[dataset]['deep']['model'].predict(x)\n",
    "        actual_fun = [actual_functions[i](xi) for xi in x]\n",
    "\n",
    "        plt.plot(x, shallow_res, \"b\", label=\"Shallow\")\n",
    "        plt.plot(x, deep_res, \"r\", label=\"Deep\")\n",
    "        plt.plot(x, actual_fun, \"g\", label=\"Actual function\")\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2\n",
    "with open('results_experiments_one.dictionary', 'wb') as config_dictionary_file:\n",
    " \n",
    "    # Step 3\n",
    "    pickle.dump(result_networks_n, config_dictionary_file)\n",
    "\n",
    "# Step 2\n",
    "with open('results_experiments_two.dictionary', 'wb') as config_dictionary_file:\n",
    " \n",
    "    # Step 3\n",
    "    pickle.dump(result_networks_custom, config_dictionary_file)\n",
    "\n",
    "# Step 2\n",
    "with open('results_experiments_three.dictionary', 'wb') as config_dictionary_file:\n",
    " \n",
    "    # Step 3\n",
    "    pickle.dump(result_networks_new_v, config_dictionary_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
